#version: "3.9"
services:


  data-init:
    build:
      context: .
      dockerfile: web.Dockerfile
    user: "0:0"
    entrypoint:
      - /bin/sh
      - -lc
      - |
        set -euo pipefail
        cd /app/site

        # Migrate quietly
        python manage.py migrate --noinput --verbosity 0

        # Seed defaults quietly (ignore duplicates on re-run)
        python - <<'PY'
        import os, sys, django, traceback
        os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'openmoxie.settings')
        django.setup()
        from django.core import management
        try:
            management.call_command('init_data', verbosity=0)
        except Exception:
            # Only print on error so CI/users see why it failed
            traceback.print_exc()
            sys.exit(1)
        PY

        # Collect static quietly
        python manage.py collectstatic --noinput --verbosity 0 || true
    environment:
      DJANGO_SETTINGS_MODULE: "openmoxie.settings"
      DEBUG: "False"
      # (Optional) hide that Django startup RuntimeWarning you were seeing:
      # PYTHONWARNINGS: "ignore::RuntimeWarning"
    volumes:
      - ./:/app
      - ./site/work:/app/site/work       # same DB/logs volume as web
      - ./site/static:/app/site/static   # optional, harmless
    restart: "no"


  model-init:
    image: curlimages/curl:8.10.1
    user: "0:0"
    volumes:
      - ./site/services/stt/models:/models
    environment:
      MODELS_DIR: "/models"
      MODEL_NAMES: "faster-whisper-medium"
    entrypoint:
      - /bin/sh
      - -lc
      - |
        set -eu
        echo "[model-init] MODELS_DIR=$$MODELS_DIR"
        echo "[model-init] MODEL_NAMES=$$MODEL_NAMES"

        # turn "a,b" into "a b"
        models=$(echo "$$MODEL_NAMES" | tr ',' ' ')

        url() {
          case "$$1" in
            faster-whisper-large) echo "https://huggingface.co/Systran/faster-whisper-large/resolve/main" ;;
            faster-whisper-large-v3) echo "https://huggingface.co/Systran/faster-whisper-large-v3/resolve/main" ;;
            faster-whisper-medium) echo "https://huggingface.co/Systran/faster-whisper-medium/resolve/main" ;;
            faster-whisper-small) echo "https://huggingface.co/Systran/faster-whisper-small/resolve/main" ;;
            faster-whisper-small.en) echo "https://huggingface.co/Systran/faster-whisper-small.en/resolve/main" ;;
            faster-whisper-base.en)  echo "https://huggingface.co/Systran/faster-whisper-base.en/resolve/main" ;;
            *) echo "" ;;
          esac
        }

        fetch() {
          name="$$1"
          base="$$2"
          out="$$MODELS_DIR/$$name"
          mkdir -p "$$out"

          required="model.bin config.json tokenizer.json"
          optional="vocabulary.json"

          need=0
          for f in $$required; do
            [ -f "$$out/$$f" ] || need=1
          done
          if [ "$$need" -eq 0 ]; then
            echo "✓ $$name already present"
            return 0
          fi

          echo "→ downloading $$name to $$out"
          for f in $$required; do
            curl -fSL "$$base/$$f" -o "$$out/$$f"
          done

          for f in $$optional; do
            if curl -fSL "$$base/$$f" -o "$$out/$$f"; then
              echo "  + $$f"
            else
              echo "  ! optional $$f not available, skipping"
            fi
          done

          echo "✓ $$name done"
        }

        for m in $$models; do
          base="$(url "$$m")"
          [ -n "$$base" ] || { echo "Unknown model $$m"; exit 1; }
          fetch "$$m" "$$base"
        done

        echo "[model-init] Seeding complete."
    restart: "no"


  stt:
    build:
      context: .
      dockerfile: stt.Dockerfile
    ports:
      - "8001:8001"
    environment:
      STT_MODEL: /models/faster-whisper-medium
      STT_DEVICE: "cuda"
      STT_COMPUTE: "float16"
      STT_VAD: "1"
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
    restart: unless-stopped
    gpus: all
    runtime: nvidia
    depends_on:
      model-init:
        condition: service_completed_successfully
    volumes:
      - ./site/services/stt/models:/models:ro   # stt reads the seeded models

  mqtt:
    build:
      context: .
      dockerfile: mqtt.Dockerfile
    ports:
      - "8883:8883"
    volumes:
      - ./keys:/keys:ro
      - ./local/work:/work
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    #ports: ["11434:11434"] 
    volumes: [ "ollama:/root/.ollama" ]
    restart: unless-stopped
    runtime: nvidia
    gpus: all
    
  ollama-init:
    image: curlimages/curl:8.10.1
    depends_on:
      ollama:
        condition: service_started
    #volumes:
    #  - ./docker/seed-ollama.sh:/seed-ollama.sh:ro
    environment:
      OLLAMA_HOST: "http://ollama:11434"
      OLLAMA_MODELS: "llama3.2:3b,deepseek-r1:1.5b"
    entrypoint:
      - /bin/sh
      - -lc
      - |
        set -eu
        echo "[ollama-init] Host: $$OLLAMA_HOST"
        echo "[ollama-init] Models: $$OLLAMA_MODELS"
        # wait for API to come up
        i=0; until curl -fsS "$${OLLAMA_HOST%/}/api/tags" >/dev/null 2>&1 || [ "$$i" -ge 180 ]; do i=$$((i+1)); sleep 2; done
        [ "$$i" -lt 180 ] || { echo "[ollama-init] Timeout waiting for $$OLLAMA_HOST" >&2; exit 1; }
        models=$$(echo "$$OLLAMA_MODELS" | tr ',' ' ')
        for m in $$models; do
          echo "→ pulling $$m"
          curl -fsS -N -X POST -H 'Content-Type: application/json' \
            -d "{\"name\":\"$$m\"}" "$${OLLAMA_HOST%/}/api/pull" \
            || { echo "pull failed for $$m" >&2; exit 1; }
          echo "✓ $$m"
        done
        echo "[ollama-init] All models pulled."

    restart: "no"


  web:
    build:
      context: .
      dockerfile: web.Dockerfile
    env_file: .env
    environment:
      DJANGO_SETTINGS_MODULE: openmoxie.settings
      STT_URL: http://stt:8001/stt
      OLLAMA_HOST: http://ollama:11434
      LLM_PROVIDER: ollama
      OLLAMA_MODEL: llama3.2:3b
      STT_BACKEND: local
      STT_LANG: es
      MQTT_HOST: mqtt
      MQTT_PORT: "8883"
      WEB_WORKERS: "1"
    ports:
      - "8000:8000"
    volumes:
      - ./site/work:/app/site/work
      - ./site/static:/app/site/static
    #depends_on:
    #  - mqtt
    #  - stt
    #  - ollama

    depends_on:
          model-init:
            condition: service_completed_successfully   
          data-init:
            condition: service_completed_successfully
          stt:
            condition: service_started
          mqtt:
            condition: service_started
          ollama:
            condition: service_started    
          ollama-init:
            condition: service_completed_successfully  
    restart: unless-stopped

volumes:
  ollama:
